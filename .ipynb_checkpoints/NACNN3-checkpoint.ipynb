{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb813a5-158b-4145-8813-e704911038bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "from abc import ABC\n",
    "\n",
    "from telegramBot import Terminator\n",
    "\n",
    "num_cores = 8\n",
    "torch.set_num_interop_threads(num_cores) # Inter-op parallelism\n",
    "torch.set_num_threads(num_cores) # Intra-op parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3042e2eb-e323-4f2e-98b3-476816779a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c3_to_c1(y):\n",
    "    if y < 2 or y > 7:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def c3_to_c2(y):\n",
    "    match y:\n",
    "        case 0:\n",
    "            return 0\n",
    "        case 1:\n",
    "            return 2\n",
    "        case 2:\n",
    "            return 3\n",
    "        case 3:\n",
    "            return 5\n",
    "        case 4:\n",
    "            return 6\n",
    "        case 5:\n",
    "            return 5\n",
    "        case 6:\n",
    "            return 4\n",
    "        case 7:\n",
    "            return 6\n",
    "        case 8:\n",
    "            return 1\n",
    "        case _:\n",
    "            return 2\n",
    "\n",
    "def c2_to_c1(y):\n",
    "    if y < 3:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc309aa-705a-4d04-844c-c0705228089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "coarser = Lambda(lambda y: torch.tensor([c3_to_c1(y), c3_to_c2(y), int(y)]))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform, target_transform = coarser)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_cores)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform, target_transform = coarser)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0ea53-ba3c-4e1e-b687-d361ce7c7c6b",
   "metadata": {},
   "source": [
    "## NA_Layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52da67f-ad0f-4bf4-a9dc-9be1bc0a4441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class NA_Layer2(ABC, nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n\\n    def forward(self, x):\\n        yf = self.layer_f(x[0, :])\\n        yi = self.layer_f(x[1, :]) + self.layer_i(x[0, :])\\n\\n        return torch.stack((yf, yi))\\n\\n    def infinitesimal_gradient(self):\\n        self.layer_i.weight.grad = self.layer_f.weight.grad\\n        self.layer_f.weight.grad = None\\n\\n        if bias:\\n            self.layer_i.bias.grad = self.layer_f.bias.grad\\n            self.layer_f.bias.grad = None\\n\\nclass NA_Linear2(NA_Layer2):\\n    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None):\\n\\n        super().__init__()\\n        self.bias = bias\\n        self.layer_f = nn.Linear(in_features, out_features, bias, device, dtype)\\n        self.layer_i = nn.Linear(in_features, out_features, bias, device, dtype)\\n\\nclass NA_Conv2d2(NA_Layer2):\\n\\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, \\n                padding_mode='zeros', device=None, dtype=None):\\n\\n        super().__init__()\\n        self.bias = bias\\n        self.layer_f = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, \\n                padding_mode, device, dtype)\\n        self.layer_i = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, \\n                padding_mode, device, dtype)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class NA_Layer2(ABC, nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        yf = self.layer_f(x[0, :])\n",
    "        yi = self.layer_f(x[1, :]) + self.layer_i(x[0, :])\n",
    "\n",
    "        return torch.stack((yf, yi))\n",
    "\n",
    "    def infinitesimal_gradient(self):\n",
    "        self.layer_i.weight.grad = self.layer_f.weight.grad\n",
    "        self.layer_f.weight.grad = None\n",
    "\n",
    "        if bias:\n",
    "            self.layer_i.bias.grad = self.layer_f.bias.grad\n",
    "            self.layer_f.bias.grad = None\n",
    "\n",
    "class NA_Linear2(NA_Layer2):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.layer_f = nn.Linear(in_features, out_features, bias, device, dtype)\n",
    "        self.layer_i = nn.Linear(in_features, out_features, bias, device, dtype)\n",
    "\n",
    "class NA_Conv2d2(NA_Layer2):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, \n",
    "                padding_mode='zeros', device=None, dtype=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.layer_f = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, \n",
    "                padding_mode, device, dtype)\n",
    "        self.layer_i = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, \n",
    "                padding_mode, device, dtype)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e30205-5405-4cd6-9c8d-4dfc2b1fc36e",
   "metadata": {},
   "source": [
    "## NA_Layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773740a6-edb6-4c6c-b808-ae622e9a2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NA_Layer(ABC, nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class NA_combiner3(NA_Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        yf = self.layer_f(x[0, :])\n",
    "        yi1 = self.layer_f(x[1, :]) + self.layer_i1(x[0, :])\n",
    "        yi2 = self.layer_f(x[2, :]) + self.layer_i1(x[1, :]) + self.layer_i2(x[0, :])\n",
    "\n",
    "        return torch.stack((yf, yi1, yi2))\n",
    "\n",
    "    def infinitesimal_gradient(self, i):\n",
    "        if i == 1:\n",
    "            self.layer_i1.weight.grad = self.layer_f.weight.grad\n",
    "            self.layer_f.weight.grad = None\n",
    "    \n",
    "            if self.bias:\n",
    "                self.layer_i1.bias.grad = self.layer_f.bias.grad\n",
    "                self.layer_f.bias.grad = None\n",
    "        \n",
    "        elif i == 2:\n",
    "            self.layer_i2.weight.grad = self.layer_f.weight.grad\n",
    "            self.layer_i1.weight.grad = None\n",
    "            self.layer_f.weight.grad = None\n",
    "    \n",
    "            if self.bias:\n",
    "                self.layer_i2.bias.grad = self.layer_f.bias.grad\n",
    "                self.layer_i1.bias.grad = None\n",
    "                self.layer_f.bias.grad = None\n",
    "            \n",
    "class NA_separate3(NA_Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        yf = self.layer_f(x[0, :])\n",
    "        yi1 = self.layer_i1(x[1, :])\n",
    "        yi2 = self.layer_i2(x[2, :])\n",
    "\n",
    "        return torch.stack((yf, yi1, yi2))\n",
    "\n",
    "class NA_independent(NA_Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = []\n",
    "        for i in np.arange(x.size(0)):\n",
    "            y.append(self.layer(x[i, :]))\n",
    "\n",
    "        return torch.stack(y)\n",
    "\n",
    "class NA_Linear3(NA_combiner3):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.layer_f = nn.Linear(in_features, out_features, bias, device, dtype)\n",
    "        self.layer_i1 = nn.Linear(in_features, out_features, bias, device, dtype)\n",
    "        self.layer_i2 = nn.Linear(in_features, out_features, bias, device, dtype)\n",
    "\n",
    "class NA_Conv2d3(NA_combiner3):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, \n",
    "                padding_mode='zeros', device=None, dtype=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.layer_f = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, \n",
    "                padding_mode, device, dtype)\n",
    "        self.layer_i1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, \n",
    "                padding_mode, device, dtype)\n",
    "        self.layer_i2 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, \n",
    "                padding_mode, device, dtype)\n",
    "\n",
    "class NA_BatchNorm2d3(NA_separate3):\n",
    "\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1, affine: bool = True,\n",
    "                 track_running_stats: bool = True, device=None, dtype=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.layer_f = nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats, device, dtype)\n",
    "        self.layer_i1 = nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats, device, dtype)\n",
    "        self.layer_i2 = nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats, device, dtype)\n",
    "\n",
    "class NA_BatchNorm1d3(NA_separate3):\n",
    "\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1, affine: bool = True,\n",
    "                 track_running_stats: bool = True, device=None, dtype=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.layer_f = nn.BatchNorm1d(num_features, eps, momentum, affine, track_running_stats, device, dtype)\n",
    "        self.layer_i1 = nn.BatchNorm1d(num_features, eps, momentum, affine, track_running_stats, device, dtype)\n",
    "        self.layer_i2 = nn.BatchNorm1d(num_features, eps, momentum, affine, track_running_stats, device, dtype)\n",
    "\n",
    "class NA_MaxPool2d(NA_independent):\n",
    "\n",
    "    def __init__(self, kernel_size, stride = None, padding = 0, dilation = 1, return_indices = False, ceil_mode = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layer = nn.MaxPool2d(kernel_size, stride, padding, dilation, return_indices, ceil_mode)\n",
    "\n",
    "class NA_Dropout(NA_independent):\n",
    "\n",
    "     def __init__(self, p: float = 0.5, inplace: bool = False):\n",
    "         \n",
    "         super().__init__()\n",
    "         self.layer = nn.Dropout(p, inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bee2f1c-fe02-4497-8102-4fbbcdceff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NA_BCNN3(nn.Module):\n",
    "    def __init__(self, learning_rate, momentum, nesterov, trainloader, testloader, \n",
    "                 epochs, num_class_c1, num_class_c2, num_class_c3, labels_c_1, labels_c_2, labels_c_3, \n",
    "                 every_print = 512, training_size = 50000):\n",
    "\n",
    "        super().__init__()\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.activation = F.relu\n",
    "        self.class_levels = 3\n",
    "        self.num_c_1 = num_class_c1\n",
    "        self.num_c_2 = num_class_c2\n",
    "        self.num_c_3 = num_class_c3\n",
    "        self.epochs = epochs\n",
    "        self.labels_c_1 = labels_c_1\n",
    "        self.labels_c_2 = labels_c_2\n",
    "        self.labels_c_3 = labels_c_3\n",
    "        self.every_print = every_print - 1 # assumed power of 2, -1 to make the mask\n",
    "        self.track_size = int( training_size / batch_size / every_print ) \n",
    "\n",
    "        self.layer11  = nn.Conv2d(3, 64, (3,3), padding = 'same')\n",
    "        self.layer12  = nn.Conv2d(3, 64, (3,3), padding = 'same')\n",
    "        self.layer13  = nn.Conv2d(3, 64, (3,3), padding = 'same')\n",
    "        self.layer2  = NA_BatchNorm2d3(64)\n",
    "        self.layer3  = NA_Conv2d3(64, 64, (3,3), padding = 'same')\n",
    "        self.layer4  = NA_BatchNorm2d3(64)\n",
    "        self.layer5  = NA_MaxPool2d((2,2), stride = (2,2))\n",
    "\n",
    "        self.layer6  = NA_Conv2d3(64, 128, (3,3), padding = 'same')\n",
    "        self.layer7  = NA_BatchNorm2d3(128)\n",
    "        self.layer8  = NA_Conv2d3(128, 128, (3,3), padding = 'same')\n",
    "        self.layer9  = NA_BatchNorm2d3(128)\n",
    "        self.layer10 = NA_MaxPool2d((2,2), stride = (2,2))\n",
    "\n",
    "        self.layerb11 = NA_Linear3(8*8*128, 256)\n",
    "        self.layerb12 = NA_BatchNorm1d3(256)\n",
    "        self.layerb13 = NA_Dropout(0.5)\n",
    "        self.layerb14 = NA_Linear3(256, 256)\n",
    "        self.layerb15 = NA_BatchNorm1d3(256)\n",
    "        self.layerb16 = NA_Dropout(0.5)\n",
    "        self.layerb17 = nn.Linear(256, self.num_c_1)\n",
    "        self.layerb18 = nn.Linear(256, self.num_c_2)\n",
    "        self.layerb19 = nn.Linear(256, self.num_c_3)\n",
    "\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr = self.learning_rate[0], \n",
    "                                   momentum = self.momentum, nesterov = self.nesterov)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        #self.criterion = nn.MSE()\n",
    "\n",
    "        self.combiner_layers = [self.layer3, self.layer6, self.layer8, self.layerb11, self.layerb14]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # block 1\n",
    "        z = torch.stack((self.layer11(x), self.layer12(x), self.layer13(x)))\n",
    "        z = self.activation(z)\n",
    "        z = self.layer2(z)\n",
    "        z = self.layer3(z)\n",
    "        z = self.activation(z)\n",
    "        z = self.layer4(z)\n",
    "        z = self.layer5(z)\n",
    "\n",
    "        # block 2\n",
    "        z = self.layer6(z)\n",
    "        z = self.activation(z)\n",
    "        z = self.layer7(z)\n",
    "        z = self.layer8(z)\n",
    "        z = self.activation(z)\n",
    "        z = self.layer9(z)\n",
    "        z = self.layer10(z)\n",
    "\n",
    "        # branch 1\n",
    "        b1 = torch.flatten(z, start_dim = 2)\n",
    "        b1 = self.layerb11(b1)\n",
    "        b1 = self.activation(b1)\n",
    "        b1 = self.layerb12(b1)\n",
    "        b1 = self.layerb13(b1)\n",
    "        b1 = self.layerb14(b1)\n",
    "        b1 = self.activation(b1)\n",
    "        b1 = self.layerb15(b1)\n",
    "        b1 = self.layerb16(b1)\n",
    "        b1_f = self.layerb17(b1[0, :])\n",
    "        b1_i1 = self.layerb18(b1[1, :])\n",
    "        b1_i2 = self.layerb19(b1[2, :])\n",
    "\n",
    "        return b1_f, b1_i1, b1_i2\n",
    "\n",
    "\n",
    "    def update_training_params(self, epoch):\n",
    "        \n",
    "        if epoch == 41:\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr = self.learning_rate[1], \n",
    "                               momentum = self.momentum, nesterov = self.nesterov)\n",
    "        elif epoch == 51:\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr = self.learning_rate[2], \n",
    "                               momentum = self.momentum, nesterov = self.nesterov)\n",
    "\n",
    "\n",
    "    def predict_and_learn(self, batch, labels):\n",
    "        self.optimizer.zero_grad()\n",
    "        predict = self(batch)\n",
    "        loss_f = self.criterion(predict[0], labels[:,0])\n",
    "        loss_i1 = self.criterion(predict[1], labels[:,1])\n",
    "        loss_i2 = self.criterion(predict[2], labels[:,2])\n",
    "\n",
    "        loss_i2.backward(retain_graph=True)\n",
    "        for l in self.combiner_layers:\n",
    "            l.infinitesimal_gradient(2)\n",
    "\n",
    "        loss_i1.backward(retain_graph=True)\n",
    "        for l in self.combiner_layers:\n",
    "            l.infinitesimal_gradient(1)\n",
    "\n",
    "        loss_f.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "\n",
    "        return torch.tensor([loss_f, loss_i1, loss_i2])\n",
    "\n",
    "\n",
    "    def train_model(self, verbose = False):\n",
    "        self.train()\n",
    "        \n",
    "        for epoch in np.arange(self.epochs):\n",
    "            self.update_training_params(epoch)\n",
    "\n",
    "            if verbose:\n",
    "                running_loss = torch.zeros(self.class_levels)\n",
    "            \n",
    "            for iter, (batch, labels) in enumerate(self.trainloader):\n",
    "                loss = self.predict_and_learn(batch, labels)\n",
    "\n",
    "                if verbose:\n",
    "                    running_loss += (loss - running_loss) / (iter+1)\n",
    "                    if (iter + 1) & self.every_print == 0:\n",
    "                        print(f'[{epoch + 1}] loss_f : {running_loss[0] :.3f}')\n",
    "                        print(f'[{epoch + 1}] loss_i1: {running_loss_[1] :.3f}')\n",
    "                        print(f'[{epoch + 1}] loss_i2: {running_loss_[2] :.3f}')\n",
    "                        for i in np.arange(self.class_levels):\n",
    "                            running_loss[i] = 0.0\n",
    "\n",
    "    \n",
    "    def train_track(self, filename = None):\n",
    "        self.train()\n",
    "        \n",
    "        self.loss_track = torch.zeros(self.epochs * self.track_size, self.class_levels)\n",
    "        self.accuracy_track = torch.zeros(self.epochs * self.track_size, self.class_levels)\n",
    "        num_push = 0\n",
    "        \n",
    "        for epoch in np.arange(self.epochs):\n",
    "\n",
    "            self.update_training_params(epoch)\n",
    "\n",
    "            running_loss = torch.zeros(self.class_levels)\n",
    "            \n",
    "            for iter, (batch, labels) in enumerate(self.trainloader):\n",
    "                loss = self.predict_and_learn(batch, labels)\n",
    "\n",
    "                running_loss += (loss - running_loss) / (iter+1)\n",
    "                if (iter + 1) & self.every_print == 0:\n",
    "                    self.loss_track[num_push, :] = running_loss\n",
    "                    self.accuracy_track[num_push, :] = self.test(mode = \"train\")\n",
    "                    num_push += 1\n",
    "                    for i in np.arange(self.class_levels):\n",
    "                            running_loss[i] = 0.0\n",
    "\n",
    "        self.plot_training_loss(filename+\"_train_loss.pdf\")\n",
    "        self.plot_test_accuracy(filename+\"_test_accuracy_.pdf\")\n",
    "\n",
    "\n",
    "    def initialize_memory(self):\n",
    "        self.correct_c1_pred = torch.zeros(self.num_c_1)\n",
    "        self.total_c1_pred = torch.zeros_like(self.correct_c1_pred)\n",
    "        \n",
    "        self.correct_c2_pred = torch.zeros(self.num_c_2)\n",
    "        self.total_c2_pred = torch.zeros_like(self.correct_c2_pred)\n",
    "        \n",
    "        self.correct_c3_pred = torch.zeros(self.num_c_3)\n",
    "        self.total_c3_pred = torch.zeros_like(self.correct_c3_pred)\n",
    "\n",
    "        self.correct_c1_vs_c2_pred = torch.zeros(self.num_c_1)\n",
    "        self.total_c1_vs_c2_pred = torch.zeros_like(self.correct_c1_vs_c2_pred)\n",
    "\n",
    "        self.correct_c2_vs_c3_pred = torch.zeros(self.num_c_2)\n",
    "        self.total_c2_vs_c3_pred = torch.zeros_like(self.correct_c2_vs_c3_pred)\n",
    "\n",
    "        self.correct_c1_vs_c3_pred = torch.zeros(self.num_c_1)\n",
    "        self.total_c1_vs_c3_pred = torch.zeros_like(self.correct_c1_vs_c3_pred)\n",
    "\n",
    "\n",
    "    def collect_test_performance(self):\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.testloader:\n",
    "                predictions = self(images)\n",
    "                predicted = torch.zeros(predictions[0].size(0), self.class_levels, dtype=torch.long)\n",
    "                _, predicted[:,0] = torch.max(predictions[0], 1)\n",
    "                _, predicted[:,1] = torch.max(predictions[1], 1)\n",
    "                _, predicted[:,2] = torch.max(predictions[2], 1)\n",
    "\n",
    "                for i in np.arange(predictions[0].size(0)):\n",
    "                    if labels[i,0] == predicted[i,0]:\n",
    "                        self.correct_c1_pred[labels[i,0]] += 1\n",
    "                        \n",
    "                    if labels[i,1] == predicted[i,1]:\n",
    "                        self.correct_c2_pred[labels[i,1]] += 1\n",
    "\n",
    "                    if labels[i,2] == predicted[i,2]:\n",
    "                        self.correct_c3_pred[labels[i,2]] += 1\n",
    "\n",
    "                    if predicted[i,1] == c3_to_c2(predicted[i,2]):\n",
    "                        self.correct_c2_vs_c3_pred[predicted[i,1]] += 1\n",
    "\n",
    "                    if predicted[i,0] == c3_to_c1(predicted[i,2]):\n",
    "                        self.correct_c1_vs_c3_pred[predicted[i,0]] += 1\n",
    "\n",
    "                    if predicted[i,0] == c2_to_c1(predicted[i,1]):\n",
    "                        self.correct_c1_vs_c2_pred[predicted[i,0]] += 1\n",
    "                        \n",
    "                    self.total_c1_pred[labels[i,0]] += 1\n",
    "                    self.total_c2_pred[labels[i,1]] += 1\n",
    "                    self.total_c3_pred[labels[i,2]] += 1\n",
    "                    self.total_c1_vs_c3_pred[predicted[i,0]] += 1\n",
    "                    self.total_c1_vs_c2_pred[predicted[i,0]] += 1\n",
    "                    self.total_c2_vs_c3_pred[predicted[i,1]] += 1\n",
    "\n",
    "    def print_test_results(self):\n",
    "        # print accuracy for each class\n",
    "        for i in np.arange(self.num_c_1):\n",
    "            accuracy_c1 = 100 * float(self.correct_c1_pred[i]) / self.total_c1_pred[i]\n",
    "            print(f'Accuracy for class {self.labels_c_1[i]:5s}: {accuracy_c1:.2f} %')\n",
    "\n",
    "        print(\"\")\n",
    "        for i in np.arange(self.num_c_2):\n",
    "            accuracy_c2 = 100 * float(self.correct_c2_pred[i]) / self.total_c2_pred[i]\n",
    "            print(f'Accuracy for class {self.labels_c_2[i]:5s}: {accuracy_c2:.2f} %')\n",
    "\n",
    "        print(\"\")\n",
    "        for i in np.arange(self.num_c_3):\n",
    "            accuracy_c3 = 100 * float(self.correct_c3_pred[i]) / self.total_c3_pred[i]\n",
    "            print(f'Accuracy for class {self.labels_c_3[i]:5s}: {accuracy_c3:.2f} %')\n",
    "            \n",
    "        # print accuracy for the whole dataset\n",
    "        print(\"\")\n",
    "        print(f'Accuracy on c1: {100 * self.correct_c1_pred.sum() // self.total_c1_pred.sum()} %')\n",
    "        print(f'Accuracy on c2: {100 * self.correct_c2_pred.sum() // self.total_c2_pred.sum()} %')\n",
    "        print(f'Accuracy on c3: {100 * self.correct_c3_pred.sum() // self.total_c3_pred.sum()} %')\n",
    "\n",
    "        # print cross classes accuracy (tree)\n",
    "        print(\"\")\n",
    "        for i in np.arange(self.num_c_1):\n",
    "            accuracy_c1_c2 = 100 * float(self.correct_c1_vs_c2_pred[i]) / self.total_c1_vs_c2_pred[i]\n",
    "            print(f'Cross-accuracy {self.labels_c_1[i]:5s} vs c2: {accuracy_c1_c2:.2f} %')\n",
    "        \n",
    "        print(\"\")\n",
    "        for i in np.arange(self.num_c_2):\n",
    "            accuracy_c2_c3 = 100 * float(self.correct_c2_vs_c3_pred[i]) / self.total_c2_vs_c3_pred[i]\n",
    "            print(f'Cross-accuracy {self.labels_c_2[i]:5s} vs c3: {accuracy_c2_c3:.2f} %')\n",
    "\n",
    "        print(\"\")\n",
    "        for i in np.arange(self.num_c_1):\n",
    "            accuracy_c1_c3 = 100 * float(self.correct_c1_vs_c3_pred[i]) / self.total_c1_vs_c3_pred[i]\n",
    "            print(f'Cross-accuracy {self.labels_c_1[i]:5s} vs c3: {accuracy_c1_c3:.2f} %')\n",
    "\n",
    "\n",
    "    def barplot(self, x, accuracy, labels, title):\n",
    "        plt.bar(x, accuracy, tick_label = labels)\n",
    "        plt.xlabel(\"Classes\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(title)\n",
    "        plt.show();\n",
    "\n",
    "    def plot_test_results(self):\n",
    "        # accuracy for each class\n",
    "        accuracy_c1 = torch.empty(self.num_c_1)\n",
    "        for i in np.arange(self.num_c_1):\n",
    "            accuracy_c1[i] = float(self.correct_c1_pred[i]) / self.total_c1_pred[i]\n",
    "        self.barplot(np.arange(self.num_c_1), accuracy_c1, self.labels_c_1, \"Accuracy on the first level\")\n",
    "\n",
    "        accuracy_c2 = torch.empty(self.num_c_2 + 1)\n",
    "        for i in np.arange(self.num_c_2):\n",
    "            accuracy_c2[i] = float(self.correct_c2_pred[i]) / self.total_c2_pred[i]\n",
    "        accuracy_c2[self.num_c_2] = self.correct_c2_pred.sum() / self.total_c2_pred.sum()\n",
    "        self.barplot(np.arange(self.num_c_2 + 1), accuracy_c2, (*self.labels_c_2, 'overall'), \"Accuracy on the second level\")\n",
    "\n",
    "        accuracy_c3 = torch.empty(self.num_c_3 + 1)\n",
    "        for i in np.arange(self.num_c_3):\n",
    "            accuracy_c3[i] = float(self.correct_c3_pred[i]) / self.total_c3_pred[i]\n",
    "        accuracy_c3[self.num_c_3] = self.correct_c3_pred.sum() / self.total_c3_pred.sum()\n",
    "        self.barplot(np.arange(self.num_c_3 + 1), accuracy_c3, (*self.labels_c_3, 'overall'), \"Accuracy on the third level\")\n",
    "\n",
    "    \n",
    "    def test(self, mode = \"print\"):\n",
    "        self.initialize_memory()\n",
    "        self.eval()\n",
    "\n",
    "        self.collect_test_performance()\n",
    "\n",
    "        match mode:\n",
    "            case \"plot\":\n",
    "                self.plot_test_results()\n",
    "            case \"print\":\n",
    "                self.print_test_results()\n",
    "            case \"train\":\n",
    "                accuracy_c1 = self.correct_c1_pred.sum() / self.total_c1_pred.sum()\n",
    "                accuracy_c2 = self.correct_c2_pred.sum() / self.total_c2_pred.sum()\n",
    "                accuracy_c3 = self.correct_c3_pred.sum() / self.total_c3_pred.sum()\n",
    "\n",
    "                self.train()\n",
    "\n",
    "                return torch.tensor([accuracy_c1, accuracy_c2, accuracy_c3])\n",
    "            case _:\n",
    "                raise AttributeError(\"Test mode not available\")\n",
    "        \n",
    "    \n",
    "    def plot_training_loss(self, filename = None):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(np.linspace(1, self.epochs, self.loss_track.size(0)), self.loss_track[:, 0].numpy(), label = \"First level\")\n",
    "        plt.plot(np.linspace(1, self.epochs, self.loss_track.size(0)), self.loss_track[:, 1].numpy(), label = \"Second level\")\n",
    "        plt.plot(np.linspace(1, self.epochs, self.loss_track.size(0)), self.loss_track[:, 2].numpy(), label = \"Third level\")\n",
    "        plt.title(\"Training loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.xticks(np.linspace(1, self.epochs, self.epochs)[0::2])\n",
    "        plt.legend()\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "        plt.show();\n",
    "\n",
    "    def plot_test_accuracy(self, filename = None):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(np.linspace(1, self.epochs, self.accuracy_track.size(0)), self.accuracy_track[:, 0].numpy(), label = \"First level\")\n",
    "        plt.plot(np.linspace(1, self.epochs, self.accuracy_track.size(0)), self.accuracy_track[:, 1].numpy(), label = \"Second level\")\n",
    "        plt.plot(np.linspace(1, self.epochs, self.accuracy_track.size(0)), self.accuracy_track[:, 2].numpy(), label = \"Third level\")\n",
    "        plt.title(\"Test accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xticks(np.linspace(1, self.epochs, self.epochs)[0::2])\n",
    "        plt.legend()\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "        plt.show();\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb179173-4f1e-4e66-8f7e-8f2d51521a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [3e-3, 5e-4, 1e-4]\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "epochs = 60\n",
    "num_class_c1 = 2\n",
    "num_class_c2 = 7\n",
    "num_class_c3 = 10\n",
    "every_print = 32\n",
    "\n",
    "#--- coarse 1 classes ---\n",
    "labels_c_1 = ('transport', 'animal')\n",
    "#--- coarse 2 classes ---\n",
    "labels_c_2 = ('sky', 'water', 'road', 'bird', 'reptile', 'pet', 'medium')\n",
    "#--- fine classes ---\n",
    "labels_c_3 = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8cb713-b124-4ccd-9748-467b99c72839",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Terminator()\n",
    "cnn = NA_BCNN3(learning_rate, momentum, nesterov, trainloader, testloader, \n",
    "                 epochs, num_class_c1, num_class_c2, num_class_c3, labels_c_1, labels_c_2, labels_c_3, every_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "474da885-cb0e-49ab-88d2-f47d66ad64de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/B-CNN3_CIFAR10_NA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_track\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     cnn\u001b[38;5;241m.\u001b[39msave_model(filename\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     cnn\u001b[38;5;241m.\u001b[39mtest(mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 165\u001b[0m, in \u001b[0;36mNA_BCNN3.train_track\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28miter\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevery_print \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_track[num_push, :] \u001b[38;5;241m=\u001b[39m running_loss\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccuracy_track[num_push, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     num_push \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_levels):\n",
      "Cell \u001b[0;32mIn[6], line 299\u001b[0m, in \u001b[0;36mNA_BCNN3.test\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_memory()\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_test_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m mode:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[6], line 197\u001b[0m, in \u001b[0;36mNA_BCNN3.collect_test_performance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestloader:\n\u001b[0;32m--> 197\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m         predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(predictions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_levels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    199\u001b[0m         _, predicted[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(predictions[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m, in \u001b[0;36mNA_BCNN3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m     59\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(z)\n\u001b[0;32m---> 60\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(z)\n\u001b[1;32m     62\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(z)\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mNA_separate3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 45\u001b[0m     yf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     yi1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_i1(x[\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     47\u001b[0m     yi2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_i2(x[\u001b[38;5;241m2\u001b[39m, :])\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/torch/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#cnn.train_model(verbose = False)\n",
    "err = False\n",
    "filename = \"models/B-CNN3_CIFAR10_NA\"\n",
    "\n",
    "try:\n",
    "    cnn.train_track(filename)\n",
    "    cnn.save_model(filename+\".pt\")\n",
    "    cnn.test(mode = \"print\")\n",
    "    \n",
    "except Exception as errore:\n",
    "    err = errore\n",
    "\n",
    "if err is False:\n",
    "    bot.sendMessage(\"Programma terminato correttamente\")\n",
    "else:\n",
    "    bot.sendMessage(\"Programma NON terminato correttamente\\nTipo di errore: \"+err.__class__.__name__+\"\\nMessaggio: \"+str(err))\n",
    "    raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7aceb1-ed26-474e-b5ae-974833ba132e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
