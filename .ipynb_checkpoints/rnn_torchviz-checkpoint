digraph {
	graph [size="19.2,19.2"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140114853327984 [label="
 (2, 10)" fillcolor=darkolivegreen1]
	140114857649840 [label=CatBackward0]
	140114851581760 -> 140114857649840
	140114851581760 [label=AddmmBackward0]
	140114856699040 -> 140114851581760
	140114854281280 [label="layer1.bias
 (2)" fillcolor=lightblue]
	140114854281280 -> 140114856699040
	140114856699040 [label=AccumulateGrad]
	140114856698704 -> 140114851581760
	140114856698704 [label=ReluBackward0]
	140114856696352 -> 140114856698704
	140114856696352 [label=AddmmBackward0]
	140114856699808 -> 140114856696352
	140114856193344 [label="layer0.bias
 (12)" fillcolor=lightblue]
	140114856193344 -> 140114856699808
	140114856699808 [label=AccumulateGrad]
	140114856699088 -> 140114856696352
	140114856699088 [label=TBackward0]
	140114856700096 -> 140114856699088
	140114862384336 [label="layer0.weight
 (12, 4)" fillcolor=lightblue]
	140114862384336 -> 140114856700096
	140114856700096 [label=AccumulateGrad]
	140114856697744 -> 140114851581760
	140114856697744 [label=TBackward0]
	140114856699376 -> 140114856697744
	140114868648224 [label="layer1.weight
 (2, 12)" fillcolor=lightblue]
	140114868648224 -> 140114856699376
	140114856699376 [label=AccumulateGrad]
	140114851587904 -> 140114857649840
	140114851587904 [label=AddBackward0]
	140114856696640 -> 140114851587904
	140114856696640 [label=AddmmBackward0]
	140114856701152 -> 140114856696640
	140114854282640 [label="layer2.bias
 (4)" fillcolor=lightblue]
	140114854282640 -> 140114856701152
	140114856701152 [label=AccumulateGrad]
	140114856700720 -> 140114856696640
	140114856700720 [label=SubBackward0]
	140114856698704 -> 140114856700720
	140114856696736 -> 140114856700720
	140114856696736 [label=MmBackward0]
	140114856697120 -> 140114856696736
	140114856697120 [label=MmBackward0]
	140114856695536 -> 140114856697120
	140114856695536 [label=PermuteBackward0]
	140114856699376 -> 140114856695536
	140114856695968 -> 140114856697120
	140114856695968 [label=MmBackward0]
	140114856696256 -> 140114856695968
	140114856696256 [label=LinalgSolveExBackward0]
	140114856696400 -> 140114856696256
	140114856696400 [label=MmBackward0]
	140114856699376 -> 140114856696400
	140114856696160 -> 140114856696400
	140114856696160 [label=PermuteBackward0]
	140114856699376 -> 140114856696160
	140114856699376 -> 140114856695968
	140114856702016 -> 140114856696640
	140114856702016 [label=TBackward0]
	140114856699328 -> 140114856702016
	140114854282720 [label="layer2.weight
 (4, 12)" fillcolor=lightblue]
	140114854282720 -> 140114856699328
	140114856699328 [label=AccumulateGrad]
	140114856699520 -> 140114851587904
	140114856699520 [label=AddmmBackward0]
	140114856699424 -> 140114856699520
	140114854282320 [label="layer2_1.bias
 (4)" fillcolor=lightblue]
	140114854282320 -> 140114856699424
	140114856699424 [label=AccumulateGrad]
	140114856696736 -> 140114856699520
	140114856696112 -> 140114856699520
	140114856696112 [label=TBackward0]
	140114856699136 -> 140114856696112
	140114854282560 [label="layer2_1.weight
 (4, 12)" fillcolor=lightblue]
	140114854282560 -> 140114856699136
	140114856699136 [label=AccumulateGrad]
	140114851593856 -> 140114857649840
	140114851593856 [label=AddBackward0]
	140114856695776 -> 140114851593856
	140114856695776 [label=AddmmBackward0]
	140114856699616 -> 140114856695776
	140114854283040 [label="layer3.bias
 (4)" fillcolor=lightblue]
	140114854283040 -> 140114856699616
	140114856699616 [label=AccumulateGrad]
	140114856700624 -> 140114856695776
	140114856700624 [label=SubBackward0]
	140114856698704 -> 140114856700624
	140114856696064 -> 140114856700624
	140114856696064 [label=MmBackward0]
	140114856695728 -> 140114856696064
	140114856695728 [label=MmBackward0]
	140114856696016 -> 140114856695728
	140114856696016 [label=PermuteBackward0]
	140114856695824 -> 140114856696016
	140114856695824 [label=CatBackward0]
	140114856699376 -> 140114856695824
	140114856699328 -> 140114856695824
	140114856695440 -> 140114856695728
	140114856695440 [label=MmBackward0]
	140114856695584 -> 140114856695440
	140114856695584 [label=LinalgSolveExBackward0]
	140114856695200 -> 140114856695584
	140114856695200 [label=MmBackward0]
	140114856695824 -> 140114856695200
	140114856695248 -> 140114856695200
	140114856695248 [label=PermuteBackward0]
	140114856695824 -> 140114856695248
	140114856695824 -> 140114856695440
	140114856697072 -> 140114856695776
	140114856697072 [label=TBackward0]
	140114856700144 -> 140114856697072
	140114854282240 [label="layer3.weight
 (4, 12)" fillcolor=lightblue]
	140114854282240 -> 140114856700144
	140114856700144 [label=AccumulateGrad]
	140114856697360 -> 140114851593856
	140114856697360 [label=AddmmBackward0]
	140114856698512 -> 140114856697360
	140114854283280 [label="layer3_1.bias
 (4)" fillcolor=lightblue]
	140114854283280 -> 140114856698512
	140114856698512 [label=AccumulateGrad]
	140114856696736 -> 140114856697360
	140114856695152 -> 140114856697360
	140114856695152 [label=TBackward0]
	140114856697216 -> 140114856695152
	140114854283440 [label="layer3_1.weight
 (4, 12)" fillcolor=lightblue]
	140114854283440 -> 140114856697216
	140114856697216 [label=AccumulateGrad]
	140114857649840 -> 140114853327984
}
