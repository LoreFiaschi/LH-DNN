usare un batch norm sulle predizioni del livello precedente per far apprendere meglio

In generale capire meglio come integrare l'informazione del livello precedente

usare norma 1 sull'ultimo layer per aumentare lo spazio di informazioni per le gerarchie successive


Considerare proiezioni più "rilassate" che possano far degradare il primo obiettivo ma non oltre una certa soglia
	tenendo conto che non è una regressione (ovvero che si deve essere il più vicino possibile ad un dato valore)
	bensì una classificazione (che non risente delle variazioni dei logits a meno che due classi non si scambino di valore).
	Quindi cercare delle proiezioni che possano sì far variare i logits ma non avvicinare troppo tra loro la prima classe
	e le altre (si potrebbe tener pure conto se appartengono o meno allo stesso branch dell'albero ma vabbé)


Capire come limitare overfitting dei livelli superiori

Usare logits dei livelli superiori come maschera per gli inferiori - oppure sommarli ai corrispondenti figli --- non funziona troppo in apprendimento


Utilizzo classificazione precedente sommandola direttamente ai logits delle classi

Non utilizzare la classificazione precedente

Usare più parametri perché è solo una questione di "rappresentabilità"

Usare blayer27 sia per ort che per prj




Verificare la correttezza dell'albeo dei label
